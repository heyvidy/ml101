{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All Activation Functions and their Transfer Derivatives\n",
    "\n",
    "# 1. Sigmoid / Logistic Function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def dsigmoid(x):\n",
    "    return x * (1-x)\n",
    "\n",
    "# 2. Rectified Linear Unit Function\n",
    "def relu(x):\n",
    "     return abs(x) * (x > 0)\n",
    "\n",
    "def drelu(x):\n",
    "     return 1. * (x > 0.)\n",
    "\n",
    "# 3. Leaky-Relu Functions\n",
    "def lrelu(x):\n",
    "    return np.where(x > 0., x, x * 0.01)\n",
    "\n",
    "def dlrelu(x):\n",
    "    dx = np.ones_like(x)\n",
    "    dx[x < 0.] = 0.01\n",
    "    return dx\n",
    "\n",
    "# 4. Hyperbolic Tan Function\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def dtanh(x):\n",
    "    return 1.0 - (np.power(np.tanh(x),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feed_forward(data_in, w0,w1,w2, w3, b0,b1,b2,b3):\n",
    "    '''\n",
    "    The Feed-forward considers 5 layers including input and output layer.\n",
    "    \n",
    "    The output layer/neuron is a classification node.\n",
    "    \n",
    "    returns: state of each layer\n",
    "    '''\n",
    "    layer0 = data_in\n",
    "    layer1 = tanh(np.dot(layer0, w0)+b0)\n",
    "    layer2 = tanh(np.dot(layer1, w1)+b1)\n",
    "    layer3 = tanh(np.dot(layer2, w2)+b2)\n",
    "    layer4 = sigmoid(np.dot(layer3, w3)+b3)\n",
    "\n",
    "    return layer0, layer1, layer2, layer3, layer4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backpropogate(i, layer0, layer1, layer2, layer3, layer4, actual_y, w0,w1,w2,w3,b0,b1,b2,b3, learning_rate):\n",
    "    '''\n",
    "    This backpropogate is only slightly different from a regular classifier\n",
    "    in ways in which the output layer gradient is calculated.\n",
    "    \n",
    "    Since the output layer is not a function of any activation function,\n",
    "    the delta doesn't need to be multiplied with the transfer derivative of the\n",
    "    output layer.\n",
    "    \n",
    "    The rest is all the same.\n",
    "    \n",
    "    returns: weights and bias matrices\n",
    "    '''\n",
    "    l4_error = layer4 - actual_y\n",
    "    l4_delta = l4_error * dsigmoid(layer4)\n",
    "    dh4 = np.dot(layer3.T, l4_delta)\n",
    "    \n",
    "    l3_error = l4_delta.dot(w3.T)\n",
    "    l3_delta = l3_error * dtanh(layer3)\n",
    "    dh3 = np.dot(layer2.T, l3_delta)\n",
    "    \n",
    "    l2_error = l3_delta.dot(w2.T)\n",
    "    l2_delta = l2_error * dtanh(layer2)\n",
    "    dh2 = np.dot(layer1.T, l2_delta)\n",
    "    \n",
    "    l1_error = l2_delta.dot(w1.T)\n",
    "    l1_delta = l1_error * dtanh(layer1)\n",
    "    dh1 = np.dot(layer0.T, l1_delta)\n",
    "    \n",
    "    w3 = w3 - (learning_rate * dh4)\n",
    "    w2 = w2 - (learning_rate * dh3)\n",
    "    w1 = w1 - (learning_rate * dh2)\n",
    "    w0 = w0 - (learning_rate * dh1)\n",
    "    \n",
    "    b3 = b3 - (learning_rate * np.mean(l4_delta))\n",
    "    b2 = b2 - (learning_rate * np.mean(l3_delta))\n",
    "    b1 = b1 - (learning_rate * np.mean(l2_delta))\n",
    "    b0 = b0 - (learning_rate * np.mean(l1_delta))    \n",
    "   \n",
    "    if i%10==0 and (i!=0):\n",
    "        loss = np.mean(np.power(layer4-actual_y, 2))\n",
    "        loss_curve.append(loss)\n",
    "        iters.append(int(i))\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print(\"\\n\", int(i), loss)\n",
    "\n",
    "        \n",
    "    return w0, w1,w2,w3, b0,b1,b2,b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(testx, testy):\n",
    "    correct = 0\n",
    "    layer0, layer1, layer2, layer3, layer4 = feed_forward(testx,w0, w1,w2,w3, b0,b1,b2,b3)\n",
    "    for i in range(len(testx)):\n",
    "        if np.argmax(layer4[i]) == np.argmax(testy[i]):\n",
    "            correct +=1 \n",
    "            \n",
    "    return f\"Accuracy: {correct*100/len(testy)}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on Wine Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine = load_wine()\n",
    "features = wine.data\n",
    "target = wine.target\n",
    "\n",
    "nt = []\n",
    "for i in target:\n",
    "    op = [0,0,0]\n",
    "    op[i] = 1\n",
    "    nt.append(op)\n",
    "    \n",
    "target = np.array(nt)\n",
    "\n",
    "X = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "Y = target\n",
    "\n",
    "X = (X-X.min()) / (X.max()-X.min())\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X.values,Y, test_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w0 = np.random.random((13,50))\n",
    "w1 = np.random.random((50,30))\n",
    "w2 = np.random.random((30, 5))\n",
    "w3 = np.random.random((5,3))\n",
    "\n",
    "b0 = np.random.random((1,1))-1\n",
    "b1 = np.random.random((1,1))-1\n",
    "b2 = np.random.random((1,1))-1\n",
    "b3 = np.random.random((1,1))-1\n",
    "\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialising variables to track loss vs iterations so we can plot the changes\n",
    "loss_curve = []\n",
    "iters = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088d8d9193c34bd6a0c453a9ff268238"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 100 0.17762828093448116\n",
      "\n",
      " 200 0.1265753285343986\n",
      "\n",
      " 300 0.07261153144087582\n",
      "\n",
      " 400 0.05921573192186994\n",
      "\n",
      " 500 0.03603881991852976\n",
      "\n",
      " 600 0.034619527539662784\n",
      "\n",
      " 700 0.021984129007542656\n",
      "\n",
      " 800 0.010756783304421489\n",
      "\n",
      " 900 0.007398253601853093\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm_notebook(range(epochs)):\n",
    "    layer0, layer1, layer2, layer3, layer4 = feed_forward(xtrain, w0,w1,w2, w3, b0,b1,b2,b3)\n",
    "    w0,w1,w2, w3, b0,b1,b2,b3 = backpropogate(i,layer0, layer1, layer2, layer3, layer4, ytrain, w0,w1,w2, w3, b0,b1,b2,b3, 0.005 )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x121662ee438>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121662712b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(iters, loss_curve,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy: 100.0%'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy: 97.9020979020979%'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
